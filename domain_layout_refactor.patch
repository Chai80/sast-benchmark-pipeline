diff --git a/pipeline/analysis/io/discovery.py b/pipeline/analysis/io/discovery.py
index a06233b..71b431e 100644
--- a/pipeline/analysis/io/discovery.py
+++ b/pipeline/analysis/io/discovery.py
@@ -1,94 +1,26 @@
-from __future__ import annotations
-
-import re
-from pathlib import Path
-from typing import Optional
-
-# Current:  YYYYMMDDNNHHMMSS (16 digits)
-# Legacy:   YYYYMMDDNN       (10 digits)
-_RUN_ID_RE = re.compile(r"^\d{10}(\d{6})?$")
-
-
-def _discover_repo_dir(tool_dir: Path, repo_name: Optional[str]) -> Optional[Path]:
-    """Discover the per-repo directory under a tool directory (legacy layout).
-
-    Legacy layout:
-      <runs_dir>/<tool>/<repo_name>/<run_id>/...
-
-    In suite layout, the repo layer is usually flattened away and tool_dir
-    contains run_id folders directly.
-    """
-    if not tool_dir.exists() or not tool_dir.is_dir():
-        return None
-
-    if repo_name:
-        p = tool_dir / repo_name
-        if p.exists() and p.is_dir():
-            return p
-
-    # Fallback: if only one directory exists, assume it's the repo dir.
-    dirs = [d for d in tool_dir.iterdir() if d.is_dir()]
-    if len(dirs) == 1:
-        return dirs[0]
+"""pipeline.analysis.io.discovery
 
-    # Case-insensitive match
-    if repo_name:
-        for d in dirs:
-            if d.name.lower() == repo_name.lower():
-                return d
+Compatibility wrapper around the canonical layout helpers.
 
-    return None
+Historically the analysis layer maintained its own implementation for finding
+the "latest" run directory and locating the corresponding normalized output.
+That created duplicated heuristics across:
 
+* tools/* (writing runs)
+* pipeline/layout.py (suite/case helper)
+* pipeline/analysis/io/discovery.py (analysis reader)
 
-def find_latest_run_dir(*, runs_dir: Path, tool: str, repo_name: str) -> Path:
-    """Find the latest run directory for tool+repo.
+The canonical implementation now lives in :mod:`sast_benchmark.io.layout`.
+This module remains as a thin shim to avoid churn in the analysis code.
+"""
 
-    Supports both layouts:
-
-    v2 (suite/case preferred):
-      <runs_dir>/<tool>/<run_id>/...
-
-    v1 (legacy):
-      <runs_dir>/<tool>/<repo_name>/<run_id>/...
-    """
-    runs_dir = Path(runs_dir)
-    tool_dir = runs_dir / tool
-    if not tool_dir.exists():
-        raise FileNotFoundError(f"Tool directory not found: {tool_dir}")
-
-    # v2: tool_dir has run_id folders directly
-    run_dirs = [d for d in tool_dir.iterdir() if d.is_dir() and _RUN_ID_RE.match(d.name)]
-    if run_dirs:
-        return max(run_dirs, key=lambda p: p.name)
-
-    # v1: tool_dir/<repo_name>/<run_id>
-    repo_dir = _discover_repo_dir(tool_dir, repo_name)
-    if repo_dir:
-        run_dirs = [d for d in repo_dir.iterdir() if d.is_dir() and _RUN_ID_RE.match(d.name)]
-        if run_dirs:
-            return max(run_dirs, key=lambda p: p.name)
-
-    raise FileNotFoundError(
-        f"No run_id folders found for tool={tool!r} repo={repo_name!r} under {tool_dir}"
-    )
-
-
-def find_latest_normalized_json(*, runs_dir: Path, tool: str, repo_name: str) -> Path:
-    """Find the latest normalized JSON for tool+repo."""
-    run_dir = find_latest_run_dir(runs_dir=runs_dir, tool=tool, repo_name=repo_name)
+from __future__ import annotations
 
-    candidates = [
-        run_dir / "normalized.json",
-        run_dir / f"{repo_name}.normalized.json",
-        run_dir / f"{repo_name}.normalized.json".replace("-", "_"),  # legacy safety
-    ]
-    for p in candidates:
-        if p.exists() and p.is_file():
-            return p
+from pathlib import Path
 
-    # Last-resort: any *.normalized.json in the run dir
-    norm_files = sorted(run_dir.glob("*.normalized.json"))
-    if norm_files:
-        return norm_files[0]
+from sast_benchmark.io.layout import find_latest_normalized_json, find_latest_run_dir
 
-    raise FileNotFoundError(f"Normalized JSON not found in: {run_dir}")
+__all__ = [
+    "find_latest_run_dir",
+    "find_latest_normalized_json",
+]
diff --git a/pipeline/layout.py b/pipeline/layout.py
index aa2db70..3f439bf 100644
--- a/pipeline/layout.py
+++ b/pipeline/layout.py
@@ -26,10 +26,14 @@ Notes
 
 from __future__ import annotations
 
-import re
 from pathlib import Path
 from typing import Optional, Union
 
+from sast_benchmark.io.layout import (
+    discover_latest_run_dir as _discover_latest_run_dir,
+    discover_repo_dir as _discover_repo_dir,
+)
+
 from pipeline.bundles import (
     BundlePaths as SuitePaths,
     ensure_bundle_dirs as ensure_suite_dirs,
@@ -41,13 +45,6 @@ from pipeline.bundles import (
 )
 
 
-# Run ids are directories created by tools/core.create_run_dir().
-#
-# Current:  YYYYMMDDNNHHMMSS (16 digits)
-# Legacy:   YYYYMMDDNN       (10 digits)
-_RUN_ID_RE = re.compile(r"^\d{10}(\d{6})?$")
-
-
 def new_suite_id() -> str:
     """Generate a new suite id (sortable UTC timestamp)."""
     return new_bundle_id()
@@ -77,61 +74,10 @@ def resolve_case_dir(
 
 
 def discover_repo_dir(output_root: Path, prefer: Optional[str] = None) -> Optional[Path]:
-    """Discover the per-tool *run root* directory inside a case.
-
-    Supports two layouts (historical compatibility):
-
-    v2 (preferred):
-      <output_root>/<run_id>/...
-
-    v1 (legacy):
-      <output_root>/<repo_name>/<run_id>/...
-
-    Parameters
-    ----------
-    output_root:
-        The tool output directory for a given case, e.g.:
-          <case_dir>/tool_runs/semgrep
-    prefer:
-        If the legacy layout contains multiple repo folders, prefer this name.
-
-    Returns
-    -------
-    Path | None
-        The directory under which run_id folders exist.
-    """
-    if not output_root.exists() or not output_root.is_dir():
-        return None
-
-    # v2: output_root contains run_id directories directly.
-    run_dirs = [d for d in output_root.iterdir() if d.is_dir() and _RUN_ID_RE.match(d.name)]
-    if run_dirs:
-        return output_root
-
-    # v1: output_root contains repo folder(s); prefer an exact match if provided.
-    if prefer:
-        p = output_root / prefer
-        if p.exists() and p.is_dir():
-            return p
-
-    dirs = [d for d in output_root.iterdir() if d.is_dir()]
-    if len(dirs) == 1:
-        return dirs[0]
-
-    # If multiple, try to find a case-insensitive match
-    if prefer:
-        for d in dirs:
-            if d.name.lower() == prefer.lower():
-                return d
-
-    return None
+    """Backwards-compatible wrapper for :func:`sast_benchmark.io.layout.discover_repo_dir`."""
+    return _discover_repo_dir(output_root, prefer)
 
 
 def discover_latest_run_dir(repo_dir: Path) -> Optional[Path]:
-    """Return the latest run directory (YYYYMMDDNN) under repo_dir."""
-    if not repo_dir.exists() or not repo_dir.is_dir():
-        return None
-    run_dirs = [d for d in repo_dir.iterdir() if d.is_dir() and _RUN_ID_RE.match(d.name)]
-    if not run_dirs:
-        return None
-    return max(run_dirs, key=lambda p: p.name)
+    """Backwards-compatible wrapper for :func:`sast_benchmark.io.layout.discover_latest_run_dir`."""
+    return _discover_latest_run_dir(repo_dir)
diff --git a/sast_benchmark/__init__.py b/sast_benchmark/__init__.py
new file mode 100644
index 0000000..29d1d21
--- /dev/null
+++ b/sast_benchmark/__init__.py
@@ -0,0 +1,20 @@
+"""sast_benchmark
+
+New package namespace for the benchmark pipeline.
+
+Why this exists
+---------------
+This repository historically organized code under top-level packages like
+``tools`` and ``pipeline``.
+
+As the project grows, it becomes valuable to establish a single "core" package
+that owns:
+
+* domain types (the canonical data contracts across tools)
+* IO/layout rules (filesystem contracts across tools and analysis)
+
+The goal is to make the CLI and tool entrypoints *thin composition roots* that
+wire together reusable components.
+"""
+
+from __future__ import annotations
diff --git a/sast_benchmark/domain/__init__.py b/sast_benchmark/domain/__init__.py
new file mode 100644
index 0000000..910b435
--- /dev/null
+++ b/sast_benchmark/domain/__init__.py
@@ -0,0 +1,19 @@
+"""sast_benchmark.domain
+
+Domain objects that form the *contract* between pipeline stages.
+
+Key idea
+--------
+Tools produce raw outputs in tool-specific formats. The benchmark pipeline
+normalizes those into a tool-agnostic representation so that analysis/scoring
+does not need to know vendor quirks.
+"""
+
+from __future__ import annotations
+
+from .finding import FindingNormalized, OwaspTop10Block
+
+__all__ = [
+    "FindingNormalized",
+    "OwaspTop10Block",
+]
diff --git a/sast_benchmark/domain/finding.py b/sast_benchmark/domain/finding.py
new file mode 100644
index 0000000..68f4765
--- /dev/null
+++ b/sast_benchmark/domain/finding.py
@@ -0,0 +1,280 @@
+"""sast_benchmark.domain.finding
+
+Canonical representation of a *normalized* finding.
+
+This is intentionally **tool-agnostic**. Every tool normalizer should be able to
+output findings that can be expressed using this structure.
+
+Why dataclasses instead of untyped dicts?
+----------------------------------------
+The repository historically used plain ``dict`` objects that follow the schema
+documented in :file:`normalized-schema.md`. That works, but it makes schema
+drift easy:
+
+* one tool starts emitting a new field name
+* another tool omits a field
+* downstream analysis silently produces wrong counts
+
+Introducing an explicit domain type provides:
+
+* a single source of truth for required fields
+* a central place for light validation
+* a predictable conversion boundary to/from JSON
+
+This file is designed to be adopted incrementally. Existing code can continue
+to read/write dicts; callers can opt into ``FindingNormalized.from_dict`` and
+``FindingNormalized.to_dict`` when ready.
+"""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Any, Dict, Iterable, List, Mapping, Optional, Tuple
+
+
+def _none_if_empty_str(v: Any) -> Any:
+    if v is None:
+        return None
+    if isinstance(v, str) and not v.strip():
+        return None
+    return v
+
+
+def _safe_int(v: Any) -> Optional[int]:
+    if v is None:
+        return None
+    # bool is a subclass of int; treat as invalid.
+    if isinstance(v, bool):
+        return None
+    try:
+        return int(v)
+    except Exception:
+        return None
+
+
+def _as_list_of_str(v: Any) -> List[str]:
+    if v is None:
+        return []
+    if isinstance(v, (list, tuple, set)):
+        return [str(x) for x in v if x is not None and str(x).strip()]
+    return [str(v)] if str(v).strip() else []
+
+
+@dataclass(frozen=True)
+class OwaspTop10Block:
+    """OWASP Top 10 block as used in the normalized schema."""
+
+    codes: List[str] = field(default_factory=list)
+    categories: List[str] = field(default_factory=list)
+
+    @classmethod
+    def from_dict(cls, d: Mapping[str, Any]) -> "OwaspTop10Block":
+        if not isinstance(d, Mapping):
+            return cls()
+        return cls(
+            codes=_as_list_of_str(d.get("codes")),
+            categories=_as_list_of_str(d.get("categories")),
+        )
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "codes": list(self.codes or []),
+            "categories": list(self.categories or []),
+        }
+
+
+@dataclass(frozen=True)
+class FindingNormalized:
+    """Tool-agnostic normalized finding.
+
+    Fields follow :file:`normalized-schema.md`.
+    """
+
+    # ---- Identity ----
+    finding_id: str
+
+    # ---- Rule + message ----
+    rule_id: Optional[str]
+    title: Optional[str]
+    severity: Optional[str] = None
+
+    # ---- Location ----
+    file_path: Optional[str] = None
+    line_number: Optional[int] = None
+    end_line_number: Optional[int] = None
+    line_content: Optional[str] = None
+
+    # ---- Optional normalized classification ----
+    issue_type: Optional[str] = None
+    vuln_class: Optional[str] = None
+    cwe_id: Optional[str] = None
+    cwe_ids: List[str] = field(default_factory=list)
+
+    owasp_top_10_2017: Optional[OwaspTop10Block] = None
+    owasp_top_10_2021: Optional[OwaspTop10Block] = None
+    owasp_top_10_2017_vendor: Optional[OwaspTop10Block] = None
+    owasp_top_10_2017_canonical: Optional[OwaspTop10Block] = None
+    owasp_top_10_2021_vendor: Optional[OwaspTop10Block] = None
+    owasp_top_10_2021_canonical: Optional[OwaspTop10Block] = None
+
+    # ---- Provenance / denormalized ----
+    metadata: Optional[Dict[str, Any]] = None
+    vendor: Optional[Dict[str, Any]] = None
+
+    # ---- Forward-compat: store unknown keys here ----
+    extra: Dict[str, Any] = field(default_factory=dict)
+
+    # Required fields for a minimally useful normalized finding.
+    REQUIRED_FIELDS: Tuple[str, ...] = (
+        "finding_id",
+        "rule_id",
+        "title",
+    )
+
+    # Fields analysis frequently expects for SAST-style issues.
+    RECOMMENDED_FIELDS: Tuple[str, ...] = (
+        "file_path",
+        "line_number",
+        "severity",
+    )
+
+    @staticmethod
+    def validate_dict(d: Mapping[str, Any]) -> List[str]:
+        """Return a list of human-readable problems for this dict.
+
+        This is intentionally lightweight: the pipeline often wants to continue
+        running even if one tool output is malformed.
+        """
+        problems: List[str] = []
+        if not isinstance(d, Mapping):
+            return ["not_a_mapping"]
+        for k in FindingNormalized.REQUIRED_FIELDS:
+            if k not in d or d.get(k) in (None, ""):
+                problems.append(f"missing:{k}")
+
+        sev = d.get("severity")
+        if sev not in (None, "", "HIGH", "MEDIUM", "LOW"):
+            problems.append("invalid:severity")
+
+        ln = d.get("line_number")
+        if ln is not None and _safe_int(ln) is None:
+            problems.append("invalid:line_number")
+
+        eln = d.get("end_line_number")
+        if eln is not None and _safe_int(eln) is None:
+            problems.append("invalid:end_line_number")
+
+        return problems
+
+    @classmethod
+    def from_dict(cls, d: Mapping[str, Any]) -> "FindingNormalized":
+        """Parse a dict (as emitted in normalized.json) into a dataclass."""
+        if not isinstance(d, Mapping):
+            raise TypeError(f"FindingNormalized.from_dict expected mapping, got {type(d)!r}")
+
+        known_keys = {
+            "finding_id",
+            "rule_id",
+            "title",
+            "severity",
+            "file_path",
+            "line_number",
+            "end_line_number",
+            "line_content",
+            "issue_type",
+            "vuln_class",
+            "cwe_id",
+            "cwe_ids",
+            "owasp_top_10_2017",
+            "owasp_top_10_2021",
+            "owasp_top_10_2017_vendor",
+            "owasp_top_10_2017_canonical",
+            "owasp_top_10_2021_vendor",
+            "owasp_top_10_2021_canonical",
+            "metadata",
+            "vendor",
+        }
+
+        extra = {k: v for k, v in d.items() if k not in known_keys}
+
+        def _owasp_block(key: str) -> Optional[OwaspTop10Block]:
+            raw = d.get(key)
+            if raw is None:
+                return None
+            if isinstance(raw, Mapping):
+                blk = OwaspTop10Block.from_dict(raw)
+                # Treat empty block as None for cleanliness.
+                if not blk.codes and not blk.categories:
+                    return None
+                return blk
+            return None
+
+        return cls(
+            finding_id=str(d.get("finding_id") or ""),
+            rule_id=_none_if_empty_str(d.get("rule_id")),
+            title=_none_if_empty_str(d.get("title")),
+            severity=_none_if_empty_str(d.get("severity")),
+            file_path=_none_if_empty_str(d.get("file_path")),
+            line_number=_safe_int(d.get("line_number")),
+            end_line_number=_safe_int(d.get("end_line_number")),
+            line_content=_none_if_empty_str(d.get("line_content")),
+            issue_type=_none_if_empty_str(d.get("issue_type")),
+            vuln_class=_none_if_empty_str(d.get("vuln_class")),
+            cwe_id=_none_if_empty_str(d.get("cwe_id")),
+            cwe_ids=_as_list_of_str(d.get("cwe_ids")),
+            owasp_top_10_2017=_owasp_block("owasp_top_10_2017"),
+            owasp_top_10_2021=_owasp_block("owasp_top_10_2021"),
+            owasp_top_10_2017_vendor=_owasp_block("owasp_top_10_2017_vendor"),
+            owasp_top_10_2017_canonical=_owasp_block("owasp_top_10_2017_canonical"),
+            owasp_top_10_2021_vendor=_owasp_block("owasp_top_10_2021_vendor"),
+            owasp_top_10_2021_canonical=_owasp_block("owasp_top_10_2021_canonical"),
+            metadata=d.get("metadata") if isinstance(d.get("metadata"), dict) else None,
+            vendor=d.get("vendor") if isinstance(d.get("vendor"), dict) else None,
+            extra=extra,
+        )
+
+    def to_dict(self) -> Dict[str, Any]:
+        """Convert to a JSON-serializable dict (normalized schema keys)."""
+        out: Dict[str, Any] = {
+            "finding_id": self.finding_id,
+            "rule_id": self.rule_id,
+            "title": self.title,
+            "severity": self.severity,
+            "file_path": self.file_path,
+            "line_number": self.line_number,
+            "end_line_number": self.end_line_number,
+            "line_content": self.line_content,
+        }
+
+        # Optional classification.
+        if self.issue_type is not None:
+            out["issue_type"] = self.issue_type
+        if self.vuln_class is not None:
+            out["vuln_class"] = self.vuln_class
+        if self.cwe_id is not None:
+            out["cwe_id"] = self.cwe_id
+        if self.cwe_ids:
+            out["cwe_ids"] = list(self.cwe_ids)
+
+        def _put_owasp(key: str, blk: Optional[OwaspTop10Block]) -> None:
+            if blk is None:
+                return
+            if blk.codes or blk.categories:
+                out[key] = blk.to_dict()
+
+        _put_owasp("owasp_top_10_2017", self.owasp_top_10_2017)
+        _put_owasp("owasp_top_10_2021", self.owasp_top_10_2021)
+        _put_owasp("owasp_top_10_2017_vendor", self.owasp_top_10_2017_vendor)
+        _put_owasp("owasp_top_10_2017_canonical", self.owasp_top_10_2017_canonical)
+        _put_owasp("owasp_top_10_2021_vendor", self.owasp_top_10_2021_vendor)
+        _put_owasp("owasp_top_10_2021_canonical", self.owasp_top_10_2021_canonical)
+
+        if self.metadata is not None:
+            out["metadata"] = self.metadata
+        if self.vendor is not None:
+            out["vendor"] = self.vendor
+
+        # Forward-compat.
+        out.update(self.extra or {})
+
+        return out
diff --git a/sast_benchmark/io/__init__.py b/sast_benchmark/io/__init__.py
new file mode 100644
index 0000000..53cdde7
--- /dev/null
+++ b/sast_benchmark/io/__init__.py
@@ -0,0 +1,37 @@
+"""sast_benchmark.io
+
+Filesystem contracts and IO helpers.
+
+Design principle
+----------------
+The benchmark output layout is a public contract.
+
+If multiple tools implement their own "where do files go" logic, the repo
+inevitably accumulates duplicated heuristics (suite mode detection, v1/v2
+layouts, candidate filenames...). This module centralizes those rules so they
+can evolve in one place.
+"""
+
+from __future__ import annotations
+
+from .layout import (
+    RUN_ID_RE,
+    RunPaths,
+    discover_repo_dir,
+    discover_latest_run_dir,
+    find_latest_normalized_json,
+    find_latest_run_dir,
+    is_suite_mode,
+    prepare_run_paths,
+)
+
+__all__ = [
+    "RUN_ID_RE",
+    "RunPaths",
+    "discover_repo_dir",
+    "discover_latest_run_dir",
+    "find_latest_normalized_json",
+    "find_latest_run_dir",
+    "is_suite_mode",
+    "prepare_run_paths",
+]
diff --git a/sast_benchmark/io/layout.py b/sast_benchmark/io/layout.py
new file mode 100644
index 0000000..ffd9cce
--- /dev/null
+++ b/sast_benchmark/io/layout.py
@@ -0,0 +1,242 @@
+"""sast_benchmark.io.layout
+
+Canonical filesystem layout utilities.
+
+This module centralizes:
+
+* "suite mode" detection (v2 suite/case layout)
+* legacy (v1) repo/<run_id> layout compatibility
+* per-run artifact filenames (raw, normalized, metadata, logs)
+* run discovery helpers (latest run, latest normalized.json)
+
+The goal is to ensure that tools, analysis, and export code do **not**
+re-implement their own layout heuristics.
+"""
+
+from __future__ import annotations
+
+import re
+from dataclasses import dataclass
+from pathlib import Path
+from typing import Optional, Sequence, Tuple, Union
+
+from tools.core import create_run_dir_compat
+
+
+# Current:  YYYYMMDDNNHHMMSS (16 digits)
+# Legacy:   YYYYMMDDNN       (10 digits)
+RUN_ID_RE = re.compile(r"^\d{10}(\d{6})?$")
+
+
+def is_suite_mode(output_root: Union[str, Path]) -> bool:
+    """Heuristic: are we writing into a suite/case tool directory?
+
+    In suite layout, tool output roots look like:
+
+      runs/suites/<suite_id>/cases/<case_id>/tool_runs/<tool>
+
+    Historically, some older runs used ``scans`` instead of ``tool_runs``.
+    """
+    p = Path(output_root)
+    return p.parent.name in {"tool_runs", "scans"}
+
+
+@dataclass(frozen=True)
+class RunPaths:
+    """Canonical artifact paths for one tool run."""
+
+    run_dir: Path
+    raw_results: Path
+    normalized: Path
+    metadata: Path
+    log: Optional[Path] = None
+    logs_dir: Optional[Path] = None
+
+    # Backwards-compatible aliases
+    @property
+    def raw_sarif(self) -> Path:
+        return self.raw_results
+
+
+def prepare_run_paths(
+    output_root: Union[str, Path],
+    repo_name: str,
+    *,
+    raw_extension: str,
+    suite_raw_basename: str = "raw",
+    suite_normalized_name: str = "normalized.json",
+    metadata_name: str = "metadata.json",
+    suite_logs_dirname: str = "logs",
+    suite_log_filename: Optional[str] = None,
+    legacy_log_filename: Optional[str] = None,
+) -> Tuple[str, RunPaths]:
+    """Prepare per-run output paths (v2 suite/case or v1 legacy).
+
+    Parameters
+    ----------
+    output_root:
+        Tool output folder. Examples:
+        - legacy: "runs/semgrep" (tool will write runs/semgrep/<repo>/<run_id>/...)
+        - suite:  "runs/suites/<suite_id>/cases/<case_id>/tool_runs/semgrep"
+    repo_name:
+        Repo name used in legacy filenames.
+    raw_extension:
+        File extension including the dot: ".json" or ".sarif".
+    suite_log_filename:
+        If set, create logs dir and set RunPaths.log under it.
+    legacy_log_filename:
+        Optional filename for legacy mode. Can include "{repo_name}".
+
+    Returns
+    -------
+    (run_id, RunPaths)
+    """
+    ext = (raw_extension or "").strip()
+    if not ext.startswith("."):
+        ext = f".{ext}" if ext else ".json"
+
+    out_root = Path(output_root)
+    suite_mode = is_suite_mode(out_root)
+
+    if suite_mode:
+        run_id, run_dir = create_run_dir_compat(out_root)
+        raw = run_dir / f"{suite_raw_basename}{ext}"
+        norm = run_dir / suite_normalized_name
+        meta = run_dir / metadata_name
+
+        logs_dir: Optional[Path] = None
+        log_path: Optional[Path] = None
+        if suite_log_filename:
+            logs_dir = run_dir / suite_logs_dirname
+            logs_dir.mkdir(parents=True, exist_ok=True)
+            log_path = logs_dir / suite_log_filename
+
+        return run_id, RunPaths(
+            run_dir=run_dir,
+            raw_results=raw,
+            normalized=norm,
+            metadata=meta,
+            log=log_path,
+            logs_dir=logs_dir,
+        )
+
+    # legacy: output_root/<repo_name>/<run_id>/...
+    run_id, run_dir = create_run_dir_compat(out_root / repo_name)
+    raw = run_dir / f"{repo_name}{ext}"
+    norm = run_dir / f"{repo_name}.normalized.json"
+    meta = run_dir / metadata_name
+
+    log_path = None
+    if legacy_log_filename:
+        fname = legacy_log_filename.format(repo_name=repo_name)
+        log_path = run_dir / fname
+
+    return run_id, RunPaths(
+        run_dir=run_dir,
+        raw_results=raw,
+        normalized=norm,
+        metadata=meta,
+        log=log_path,
+        logs_dir=None,
+    )
+
+
+def discover_repo_dir(output_root: Path, prefer: Optional[str] = None) -> Optional[Path]:
+    """Discover the per-tool *run root* directory inside a case.
+
+    Supports two layouts:
+
+    v2 (preferred):
+      <output_root>/<run_id>/...
+
+    v1 (legacy):
+      <output_root>/<repo_name>/<run_id>/...
+    """
+    if not output_root.exists() or not output_root.is_dir():
+        return None
+
+    # v2: output_root contains run_id directories directly.
+    run_dirs = [d for d in output_root.iterdir() if d.is_dir() and RUN_ID_RE.match(d.name)]
+    if run_dirs:
+        return output_root
+
+    # v1: output_root contains repo folder(s); prefer an exact match if provided.
+    if prefer:
+        p = output_root / prefer
+        if p.exists() and p.is_dir():
+            return p
+
+    dirs = [d for d in output_root.iterdir() if d.is_dir()]
+    if len(dirs) == 1:
+        return dirs[0]
+
+    # If multiple, try to find a case-insensitive match
+    if prefer:
+        for d in dirs:
+            if d.name.lower() == prefer.lower():
+                return d
+
+    return None
+
+
+def discover_latest_run_dir(repo_dir: Path) -> Optional[Path]:
+    """Return the latest run directory under repo_dir."""
+    if not repo_dir.exists() or not repo_dir.is_dir():
+        return None
+    run_dirs = [d for d in repo_dir.iterdir() if d.is_dir() and RUN_ID_RE.match(d.name)]
+    if not run_dirs:
+        return None
+    return max(run_dirs, key=lambda p: p.name)
+
+
+def find_latest_run_dir(*, runs_dir: Path, tool: str, repo_name: str) -> Path:
+    """Find the latest run directory for tool+repo.
+
+    Supports both layouts:
+
+    v2:
+      <runs_dir>/<tool>/<run_id>/...
+
+    v1:
+      <runs_dir>/<tool>/<repo_name>/<run_id>/...
+    """
+    runs_dir = Path(runs_dir)
+    tool_dir = runs_dir / tool
+    if not tool_dir.exists():
+        raise FileNotFoundError(f"Tool directory not found: {tool_dir}")
+
+    # v2: tool_dir has run_id folders directly
+    run_dirs = [d for d in tool_dir.iterdir() if d.is_dir() and RUN_ID_RE.match(d.name)]
+    if run_dirs:
+        return max(run_dirs, key=lambda p: p.name)
+
+    # v1: tool_dir/<repo_name>/<run_id>
+    repo_dir = discover_repo_dir(tool_dir, prefer=repo_name)
+    if repo_dir:
+        run_dirs = [d for d in repo_dir.iterdir() if d.is_dir() and RUN_ID_RE.match(d.name)]
+        if run_dirs:
+            return max(run_dirs, key=lambda p: p.name)
+
+    raise FileNotFoundError(
+        f"No run_id folders found for tool={tool!r} repo={repo_name!r} under {tool_dir}"
+    )
+
+
+def find_latest_normalized_json(*, runs_dir: Path, tool: str, repo_name: str) -> Path:
+    """Find the latest normalized JSON for tool+repo."""
+    run_dir = find_latest_run_dir(runs_dir=runs_dir, tool=tool, repo_name=repo_name)
+
+    candidates: Sequence[Path] = (
+        run_dir / "normalized.json",
+        run_dir / f"{repo_name}.normalized.json",
+        run_dir / f"{repo_name}.normalized.json".replace("-", "_"),
+    )
+    for p in candidates:
+        if p.exists() and p.is_file():
+            return p
+
+    norm_files = sorted(run_dir.glob("*.normalized.json"))
+    if norm_files:
+        return norm_files[0]
+
+    raise FileNotFoundError(f"Normalized JSON not found in: {run_dir}")
diff --git a/tools/aikido/runner.py b/tools/aikido/runner.py
index eccffa9..8ef849e 100644
--- a/tools/aikido/runner.py
+++ b/tools/aikido/runner.py
@@ -31,10 +31,11 @@ from typing import Any, Dict, List, Optional, Sequence, Tuple
 
 from dotenv import load_dotenv
 
+from sast_benchmark.io.layout import RunPaths, prepare_run_paths as _prepare_run_paths
+
 from tools.core import (
     acquire_repo,
     build_run_metadata as build_std_run_metadata,
-    create_run_dir_compat,
     get_pipeline_git_commit,
     run_cmd,
     which_or_raise,
@@ -79,43 +80,16 @@ class AikidoConfig:
     token: str
 
 
-@dataclass(frozen=True)
-class RunPaths:
-    run_dir: Path
-    raw_results: Path
-    normalized: Path
-    metadata: Path
-
-
 LOCAL_SCANNER_DOCKER_IMAGE_DEFAULT = "aikidosecurity/local-scanner:latest"
 
 
 def prepare_run_paths(output_root: str, repo_name: str) -> Tuple[str, RunPaths]:
     """Prepare per-run output paths.
 
-    Layouts supported:
-    - v2 (suite/case): <output_root>/<run_id>/{raw.json,normalized.json,metadata.json}
-      where output_root is cases/<case>/tool_runs/<tool>
-    - v1 (legacy):     <output_root>/<repo_name>/<run_id>/{<repo>.json,<repo>.normalized.json,metadata.json}
+    Delegates to :func:`sast_benchmark.io.layout.prepare_run_paths` so the
+    filesystem contract is owned by one module.
     """
-    out_root = Path(output_root)
-    suite_mode = out_root.parent.name in {"tool_runs", "scans"}
-
-    if suite_mode:
-        run_id, run_dir = create_run_dir_compat(out_root)
-        raw = run_dir / "raw.json"
-        norm = run_dir / "normalized.json"
-    else:
-        run_id, run_dir = create_run_dir_compat(out_root / repo_name)
-        raw = run_dir / f"{repo_name}.json"
-        norm = run_dir / f"{repo_name}.normalized.json"
-
-    return run_id, RunPaths(
-        run_dir=run_dir,
-        raw_results=raw,
-        normalized=norm,
-        metadata=run_dir / "metadata.json",
-    )
+    return _prepare_run_paths(output_root, repo_name, raw_extension=".json")
 
 
 def get_aikido_config() -> AikidoConfig:
diff --git a/tools/scan_sonar.py b/tools/scan_sonar.py
index b87fc0b..f3bbe97 100644
--- a/tools/scan_sonar.py
+++ b/tools/scan_sonar.py
@@ -26,7 +26,7 @@ import re
 import subprocess
 import sys
 import time
-from dataclasses import dataclass
+from sast_benchmark.io.layout import RunPaths, prepare_run_paths as _prepare_run_paths
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
@@ -44,7 +44,6 @@ from tools.core import (
     ROOT_DIR,
     acquire_repo,
     build_run_metadata,
-    create_run_dir_compat,
     run_cmd,
     which_or_raise,
     write_json,
@@ -64,15 +63,6 @@ SONAR_HOST_DEFAULT = "https://sonarcloud.io"
 SONAR_SCANNER_FALLBACKS = ["/opt/homebrew/bin/sonar-scanner", "/usr/local/bin/sonar-scanner"]
 
 
-@dataclass(frozen=True)
-class RunPaths:
-    run_dir: Path
-    log: Path
-    raw_results: Path
-    normalized: Path
-    metadata: Path
-
-
 # ---------------------------------------------------------------------------
 # CLI
 # ---------------------------------------------------------------------------
@@ -126,33 +116,15 @@ def get_sonar_config() -> SonarConfig:
 def prepare_run_paths(output_root: str, repo_name: str) -> Tuple[str, RunPaths]:
     """Prepare per-run output paths.
 
-    Layouts supported:
-    - v2 (suite/case): <output_root>/<run_id>/{raw.json,normalized.json,metadata.json,logs/...}
-      where output_root is cases/<case>/tool_runs/<tool>
-    - v1 (legacy):     <output_root>/<repo_name>/<run_id>/{<repo>.json,<repo>.normalized.json,metadata.json}
+    Delegates to :func:`sast_benchmark.io.layout.prepare_run_paths` so the
+    filesystem contract is owned by one module.
     """
-    out_root = Path(output_root)
-    suite_mode = out_root.parent.name in {"tool_runs", "scans"}
-
-    if suite_mode:
-        run_id, run_dir = create_run_dir_compat(out_root)
-        logs_dir = run_dir / "logs"
-        logs_dir.mkdir(parents=True, exist_ok=True)
-        return run_id, RunPaths(
-            run_dir=run_dir,
-            log=logs_dir / "sonar_scan.log",
-            raw_results=run_dir / "raw.json",
-            normalized=run_dir / "normalized.json",
-            metadata=run_dir / "metadata.json",
-        )
-
-    run_id, run_dir = create_run_dir_compat(out_root / repo_name)
-    return run_id, RunPaths(
-        run_dir=run_dir,
-        log=run_dir / f"{repo_name}_sonar_scan.log",
-        raw_results=run_dir / f"{repo_name}.json",
-        normalized=run_dir / f"{repo_name}.normalized.json",
-        metadata=run_dir / "metadata.json",
+    return _prepare_run_paths(
+        output_root,
+        repo_name,
+        raw_extension=".json",
+        suite_log_filename="sonar_scan.log",
+        legacy_log_filename="{repo_name}_sonar_scan.log",
     )
 
 
diff --git a/tools/semgrep/runner.py b/tools/semgrep/runner.py
index b330b06..b71f9d9 100644
--- a/tools/semgrep/runner.py
+++ b/tools/semgrep/runner.py
@@ -6,52 +6,22 @@ Keeps Semgrep CLI quirks and run-directory layout close to the tool.
 
 from __future__ import annotations
 
-from dataclasses import dataclass
 from pathlib import Path
 from typing import Tuple
 
-from tools.core import create_run_dir_compat, run_cmd
+from sast_benchmark.io.layout import RunPaths, prepare_run_paths as _prepare_run_paths
+from tools.core import run_cmd
 
 SEMGREP_FALLBACKS = ["/opt/homebrew/bin/semgrep", "/usr/local/bin/semgrep"]
 
 
-@dataclass(frozen=True)
-class RunPaths:
-    run_dir: Path
-    raw_results: Path
-    normalized: Path
-    metadata: Path
-
-
 def prepare_run_paths(output_root: str, repo_name: str) -> Tuple[str, RunPaths]:
     """Prepare per-run output paths.
 
-    Layouts supported:
-    - v2 (suite/case): <output_root>/<run_id>/{raw.json,normalized.json,metadata.json}
-      where output_root is cases/<case>/tool_runs/<tool>
-    - v1 (legacy):     <output_root>/<repo_name>/<run_id>/{<repo>.json,<repo>.normalized.json,metadata.json}
+    Delegates to :func:`sast_benchmark.io.layout.prepare_run_paths` so the
+    filesystem contract is owned by one module.
     """
-    out_root = Path(output_root)
-
-    # In suite mode, sast_cli passes .../cases/<case>/(tool_runs|scans)/<tool>
-    # and we flatten away the redundant repo_name directory.
-    suite_mode = out_root.parent.name in {"tool_runs", "scans"}
-
-    if suite_mode:
-        run_id, run_dir = create_run_dir_compat(out_root)
-        raw = run_dir / "raw.json"
-        norm = run_dir / "normalized.json"
-    else:
-        run_id, run_dir = create_run_dir_compat(out_root / repo_name)
-        raw = run_dir / f"{repo_name}.json"
-        norm = run_dir / f"{repo_name}.normalized.json"
-
-    return run_id, RunPaths(
-        run_dir=run_dir,
-        raw_results=raw,
-        normalized=norm,
-        metadata=run_dir / "metadata.json",
-    )
+    return _prepare_run_paths(output_root, repo_name, raw_extension=".json")
 
 
 def semgrep_version(semgrep_bin: str) -> str:
diff --git a/tools/snyk/runner.py b/tools/snyk/runner.py
index 0899cc3..194d34a 100644
--- a/tools/snyk/runner.py
+++ b/tools/snyk/runner.py
@@ -6,49 +6,22 @@ Tool-specific execution plumbing for Snyk Code (SARIF).
 from __future__ import annotations
 
 import os
-from dataclasses import dataclass
 from pathlib import Path
 from typing import Tuple
 
-from tools.core import create_run_dir_compat, run_cmd
+from sast_benchmark.io.layout import RunPaths, prepare_run_paths as _prepare_run_paths
+from tools.core import run_cmd
 
 SNYK_BIN_FALLBACKS = ["/opt/homebrew/bin/snyk", "/usr/local/bin/snyk"]
 
 
-@dataclass(frozen=True)
-class RunPaths:
-    run_dir: Path
-    raw_sarif: Path
-    normalized: Path
-    metadata: Path
-
-
 def prepare_run_paths(output_root: str, repo_name: str) -> Tuple[str, RunPaths]:
     """Prepare per-run output paths.
 
-    Layouts supported:
-    - v2 (suite/case): <output_root>/<run_id>/{raw.sarif,normalized.json,metadata.json}
-      where output_root is cases/<case>/tool_runs/<tool>
-    - v1 (legacy):     <output_root>/<repo_name>/<run_id>/{<repo>.sarif,<repo>.normalized.json,metadata.json}
+    Delegates to :func:`sast_benchmark.io.layout.prepare_run_paths` so the
+    filesystem contract is owned by one module.
     """
-    out_root = Path(output_root)
-    suite_mode = out_root.parent.name in {"tool_runs", "scans"}
-
-    if suite_mode:
-        run_id, run_dir = create_run_dir_compat(out_root)
-        raw = run_dir / "raw.sarif"
-        norm = run_dir / "normalized.json"
-    else:
-        run_id, run_dir = create_run_dir_compat(out_root / repo_name)
-        raw = run_dir / f"{repo_name}.sarif"
-        norm = run_dir / f"{repo_name}.normalized.json"
-
-    return run_id, RunPaths(
-        run_dir=run_dir,
-        raw_sarif=raw,
-        normalized=norm,
-        metadata=run_dir / "metadata.json",
-    )
+    return _prepare_run_paths(output_root, repo_name, raw_extension=".sarif")
 
 
 def require_snyk_token() -> None:
