# SAST Benchmark API – Architecture & Data Model

This document describes the internal API and data model we will use to run and analyze
SAST benchmark scans across multiple tools (Semgrep, Snyk, Sonar, Aikido, etc.)
on top of the existing **sast-benchmark-pipeline** repo.

The goals of this API are:

- Run SAST scanners in a consistent way across a set of benchmark targets
- Collect **runtime + findings** per tool & target for fair comparisons
- Persist normalized data in a shape that is ready for **ML / analytics** later
- Keep v1 **simple**, avoiding over‑engineering while leaving room to grow

> This README is about the **service layer** on top of the existing CLI scripts
> and normalized JSON schema defined in `normalized-schema.md`.

---

## 1. Concepts & Goals

Each **scan** is:

- One tool (e.g. `semgrep`)
- On one repository / commit
- At one point in time
- Producing:
  - A normalized set of **findings**
  - One **run time** measurement (`scan_time_seconds`, from `metadata.json`)

The API exposes:

- Endpoints to **start scans**, **check scan status**, and **inspect results**
- A simple data model for **benchmark dashboards** and **future ML pipelines**

We intentionally avoid extra complexity (no separate Target DB table, no vendor JSON in DB) in v1.

---

## 2. High‑Level Architecture

```text
+-----------+        +------------------+         +-----------------+
|  Client   |  HTTP  |  API Service     |  Jobs   | Worker / Runner |
| (CI, UI,  |------->|  (FastAPI, etc.) |-------->| (runs scanners  |
| scripts)  |        |                  |         |  + ingest)      |
+-----------+        +------------------+         +-----------------+
                                               |
                                               v
                                        +-------------+
                                        |  SAST Tools |
                                        | (semgrep,   |
                                        |  snyk,      |
                                        |  sonar,     |
                                        |  aikido)    |
                                        +-------------+
                                               |
     writes raw JSON + metadata.json + normalized.json into repo's `runs/` tree
                                               |
                                               v
                                        +-----------------+
                                        |  Database       |
                                        |  (Scans,       |
                                        |   Findings)    |
                                        +-----------------+
```

**Flow:**

1. Client calls `POST /scans` with a `scanner` and a `target_key` (or arbitrary `repo_url`).
2. API creates a `Scan` record with status `queued`, enqueues a job (e.g. Redis/RQ or Celery).
3. Worker runs the appropriate `tools/scan_<scanner>.py` script:
   - Writes:
     - raw tool output (`<repo>.json`)
     - `metadata.json` (includes `scan_time_seconds`)
     - `<repo>.normalized.json` (schema v1.1 from `normalized-schema.md`)
4. Worker ingests normalized JSON + metadata into the database:
   - One `Scan` row
   - Many `Finding` rows
5. Client queries `GET /scans/{id}` and `GET /scans/{id}/findings` for metrics & details.

The existing `runs/` directory structure remains the **source of raw artifacts**.

---

## 3. Data Model (v1)

In v1 we keep the data model intentionally small:

- We do **not** create a `targets` table yet (targets remain config in code, e.g. `benchmarks/targets.py`)
- We do **not** store `vendor.raw_result` in the DB (only normalized fields)

### 3.1 Scan

Represents one SAST tool run on one repository/commit.

```text
Scan
----

id                    UUID (PK)
tool                  string        e.g. "semgrep", "snyk", "sonar", "aikido"
tool_version          string        version reported by the scanner

target_key            string?       optional benchmark identifier (e.g. "juice_shop")

target_repo_name      string        repo name ("juice-shop")
target_repo_url       string        repo URL
target_repo_commit    string?       git SHA (nullable if not available)
target_repo_date      string?       ISO 8601 date/time (nullable)

run_id                string        run directory ID (e.g. "2025120201")
scan_date             string        when the scan ran (ISO 8601)
command               string        full CLI command used for the scan

scan_time_seconds     float         runtime reported by the scanner's metadata.json

total_findings        integer       number of normalized findings
high_count            integer       number of HIGH severity findings
medium_count          integer       number of MEDIUM severity findings
low_count             integer       number of LOW severity findings

status                string        "queued" | "running" | "success" | "failed"
```

Most of these fields are derived from:

- `normalized.json` header (`tool`, `tool_version`, `target_repo`, `scan.*`)
- `metadata.json` (`scan_time_seconds`)
- Aggregations over `findings[]` (counts)

### 3.2 Finding

Represents one normalized issue from `findings[]`.

```text
Finding
-------

id                    UUID (PK)
scan_id               UUID (FK -> Scan.id)

finding_id            string        "<tool>:<rule_id>:<file_path>:<line>" (unique per Scan)
cwe_id                string?       e.g. "CWE-89"
rule_id               string        tool's rule identifier
title                 string        human-friendly description

severity              string?       "HIGH" | "MEDIUM" | "LOW" | null

file_path             string?       path within repo, e.g. "routes/search.ts"
line_number           integer?      1-based start line
end_line_number       integer?      1-based end line
line_content          string?       snippet of source code at line_number
```

Vendor‑specific details remain in the file system (raw tool JSON) and can be accessed
by `scan.run_id` + `finding_id` if required.

---

## 4. API Surface (v1)

We expose a small set of endpoints to keep things focused and composable.

### 4.1 Create a Scan

`POST /scans`

Request body:

```json
{
  "scanner": "semgrep",
  "target_key": "juice_shop",      // optional; if omitted, use repo_url
  "repo_url": null,                // required if target_key is omitted
  "options": {
    "extra_args": []
  }
}
```

- `scanner` is one of the supported tools, e.g. `"semgrep"`, `"snyk"`, `"sonar"`, `"aikido"`.
- `target_key` refers to a benchmark target defined in code
  (e.g. in `benchmarks/targets.py`).
- If `target_key` is omitted, `repo_url` must be provided for an ad‑hoc repo.

Response (example):

```json
{
  "id": "scan-uuid",
  "status": "queued",
  "tool": "semgrep",
  "target_key": "juice_shop"
}
```

---

### 4.2 Get Scan Metadata & Summary

`GET /scans/{scan_id}`

Response (example):

```json
{
  "id": "scan-uuid",
  "status": "success",

  "tool": "semgrep",
  "tool_version": "1.2.3",
  "target_key": "juice_shop",

  "target_repo": {
    "name": "juice-shop",
    "url": "https://github.com/juice-shop/juice-shop.git",
    "commit": "abc123",
    "commit_date": "2025-11-26T11:38:38+01:00"
  },

  "run_id": "2025120201",
  "scan_date": "2025-12-02T23:47:41.515636",
  "command": "semgrep --config ...",

  "scan_time_seconds": 15.75,

  "summary": {
    "total_findings": 42,
    "severity_counts": {
      "HIGH": 5,
      "MEDIUM": 20,
      "LOW": 17
    }
  }
}
```

This is the main endpoint used for **benchmark charts** (runtime vs findings).

---

### 4.3 List Findings for a Scan

`GET /scans/{scan_id}/findings`

Query params (optional):

- `severity` (e.g. `severity=HIGH`, can be repeated)
- `cwe_id`
- `rule_id`
- `file_path`
- `limit`, `offset`

Response (example):

```json
{
  "scan_id": "scan-uuid",
  "findings": [
    {
      "id": "finding-uuid",
      "finding_id": "semgrep:javascript/Sqli:routes/search.ts:23",
      "cwe_id": "CWE-89",
      "rule_id": "javascript/Sqli",
      "title": "Unsanitized SQL query built from user input.",
      "severity": "HIGH",
      "file_path": "routes/search.ts",
      "line_number": 23,
      "end_line_number": 23,
      "line_content": "db.query(`SELECT * ... ${req.query.id}`)"
    }
  ],
  "pagination": {
    "limit": 50,
    "offset": 0,
    "total": 42
  }
}
```

This endpoint is the main source for:

- Detailed benchmark analysis
- Exporting data for ML experiments (e.g. scripts can dump findings to CSV/Parquet)

---

### 4.4 Runtime Benchmark (Optional, v1.1+)

Later, we can add dedicated endpoints for running all tools against a target
and summarizing runtime:

- `POST /benchmarks/runtime`
- `GET /benchmarks/runtime/{benchmark_id}`

Internally, these will:

- Trigger multiple scans (one per tool) for the same target/commit.
- Aggregate `scan_time_seconds` and severity counts for comparison.

For v1, we can rely on individual scans + simple queries to compare tools.

---

## 5. Implementation Notes

- **Targets**
  - Keep benchmark targets in Python code (e.g. `benchmarks/targets.py`), exactly as today.
  - Each `Scan` stores a `target_key` string (or `null` for ad‑hoc repos).

- **Ingestion**
  - After a tool run:
    - Load `<repo>.normalized.json` to get `tool`, `tool_version`, `target_repo`, `scan`.
    - Load `metadata.json` to get `scan_time_seconds`.
    - Compute `total_findings` & severity counts from `findings[]`.
    - Insert one `Scan`, many `Findings`.

- **ML / Analytics**
  - ML jobs and analytics queries hit the same DB:
    - `Scans` gives tool, target, commit, time, runtime and counts.
    - `Findings` gives CWE, rule, severity, code snippet per issue.
  - If we later need vendor‑specific fields, we can:
    - Add `vendor_raw jsonb` to `Finding`, or
    - Store vendor blobs in object storage with a reference.

This design keeps v1 **small and understandable**, while preserving all the
important signals for SAST benchmarking and future ML or AI‑based detection work.
